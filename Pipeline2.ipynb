{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7210387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Install required libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install timm matplotlib scikit-learn xgboost optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a3d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Import libraries and configure CUDA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import os, copy, time, gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# Configure CUDA\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,expandable_segments:True'\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Verify dataset structure\n",
    "DATA_ROOT = '/workspace/VDMDR'\n",
    "if os.path.exists(os.path.join(DATA_ROOT, 'RGB')) and os.path.exists(os.path.join(DATA_ROOT, 'Vessel')):\n",
    "    print(\"Dataset structure OK\")\n",
    "    print(\"RGB classes:\", os.listdir(os.path.join(DATA_ROOT, 'RGB')))\n",
    "    print(\"Vessel classes:\", os.listdir(os.path.join(DATA_ROOT, 'Vessel')))\n",
    "else:\n",
    "    print(\"Dataset structure NOT found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e005db5-aae8-4474-b79f-c1119eeae066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - SMOTE-based dataset balancing using ResNet features\n",
    "class SMOTEImageDataset:\n",
    "    def __init__(self, dataset, target_size=(224, 224)):\n",
    "        self.dataset = dataset\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def apply_smote(self, random_state=42, k_neighbors=5):\n",
    "        features, labels = [], []\n",
    "        feature_extractor = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "        feature_extractor.fc = nn.Identity()\n",
    "        feature_extractor.eval()\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(self.target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        with torch.no_grad():\n",
    "            for rgb, _, label in self.dataset:\n",
    "                if isinstance(rgb, torch.Tensor):\n",
    "                    rgb = transforms.ToPILImage()(rgb)\n",
    "                img_tensor = transform(rgb).unsqueeze(0)\n",
    "                feature = feature_extractor(img_tensor).squeeze().numpy()\n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        print(f\"Original distribution: {Counter(labels)}\")\n",
    "        smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        X_res, y_res = smote.fit_resample(features, labels)\n",
    "        print(f\"Balanced distribution: {Counter(y_res)}\")\n",
    "        return X_res, y_res\n",
    "\n",
    "    def create_synthetic_images(self, features_balanced, labels_balanced):\n",
    "        synthetic_dataset = []\n",
    "        for i, (feature, label) in enumerate(zip(features_balanced, labels_balanced)):\n",
    "            if i < len(self.dataset):\n",
    "                synthetic_dataset.append(self.dataset[i])\n",
    "            else:\n",
    "                original_features = features_balanced[:len(self.dataset)]\n",
    "                closest_idx = np.argmin(np.linalg.norm(original_features - feature, axis=1))\n",
    "                rgb, vessel, _ = self.dataset[closest_idx]\n",
    "                if isinstance(rgb, torch.Tensor):\n",
    "                    rgb = transforms.ToPILImage()(rgb)\n",
    "                if isinstance(vessel, torch.Tensor):\n",
    "                    vessel = transforms.ToPILImage()(vessel)\n",
    "                rgb_aug = transforms.Compose([\n",
    "                    transforms.RandomRotation(15),\n",
    "                    transforms.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomAffine(0, translate=(0.1, 0.1))\n",
    "                ])(rgb)\n",
    "                vessel_aug = transforms.Compose([\n",
    "                    transforms.RandomRotation(15),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomAffine(0, translate=(0.1, 0.1))\n",
    "                ])(vessel)\n",
    "                synthetic_dataset.append((rgb_aug, vessel_aug, label))\n",
    "        return synthetic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaeccbe",
   "metadata": {
    "id": "fdaeccbe"
   },
   "outputs": [],
   "source": [
    "# 4 - Data Loading & Preprocessing (with separate transforms for RGB and Vessel)\n",
    "DATA_ROOT = '/workspace/VDMDR'\n",
    "\n",
    "class VDMDRDataset(Dataset):\n",
    "    def __init__(self, rgb_dir, vessel_dir):\n",
    "        self.rgb_paths = []\n",
    "        self.vessel_paths = []\n",
    "        self.labels = []\n",
    "        for class_folder in sorted(os.listdir(rgb_dir)):\n",
    "            rgb_class_dir = os.path.join(rgb_dir, class_folder)\n",
    "            vessel_class_dir = os.path.join(vessel_dir, class_folder)\n",
    "            if not os.path.isdir(rgb_class_dir): continue\n",
    "            for fname in sorted(os.listdir(rgb_class_dir)):\n",
    "                rgb_path = os.path.join(rgb_class_dir, fname)\n",
    "                vessel_path = os.path.join(vessel_class_dir, fname)\n",
    "                if os.path.exists(rgb_path) and os.path.exists(vessel_path):\n",
    "                    self.rgb_paths.append(rgb_path)\n",
    "                    self.vessel_paths.append(vessel_path)\n",
    "                    self.labels.append(int(class_folder))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb = Image.open(self.rgb_paths[idx]).convert('RGB')\n",
    "        vessel = Image.open(self.vessel_paths[idx]).convert('L')\n",
    "        label = self.labels[idx]\n",
    "        return rgb, vessel, label\n",
    "\n",
    "# Transforms for better aug RGB (3-channel) and Vessel (1-channel)\n",
    "rgb_train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.15))\n",
    "])\n",
    "\n",
    "rgb_val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "vessel_train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),  # For grayscale\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.15))\n",
    "])\n",
    "vessel_val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # For grayscale\n",
    "])\n",
    "\n",
    "# Paths\n",
    "RGB_DIR = os.path.join(DATA_ROOT, 'RGB')\n",
    "VESSEL_DIR = os.path.join(DATA_ROOT, 'Vessel')\n",
    "\n",
    "# Load full dataset\n",
    "full_dataset = VDMDRDataset(RGB_DIR, VESSEL_DIR)\n",
    "\n",
    "# Stratified K-Fold Cross-Validation Split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# full_dataset.labels is already available\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "folds = list(skf.split(np.arange(len(full_dataset.labels)), full_dataset.labels))\n",
    "fold_num = 0 \n",
    "train_idx, val_idx = folds[fold_num]\n",
    "train_dataset = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "val_dataset = torch.utils.data.Subset(full_dataset, val_idx)\n",
    "print(f\"Fold {fold_num}: {len(train_dataset)} train, {len(val_dataset)} val\")\n",
    "print(f\"Number of classes: {len(set(full_dataset.labels))}\")\n",
    "\n",
    "# Subset wrapper with separate transforms for RGB and Vessel\n",
    "class SubsetWithTransform(Dataset):\n",
    "    def __init__(self, dataset, indices, rgb_transform, vessel_transform):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.rgb_transform = rgb_transform\n",
    "        self.vessel_transform = vessel_transform\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        rgb, vessel, label = self.dataset[self.indices[idx]]\n",
    "        if self.rgb_transform:\n",
    "            rgb = self.rgb_transform(rgb)\n",
    "        if self.vessel_transform:\n",
    "            vessel = self.vessel_transform(vessel)\n",
    "        return rgb, vessel, label\n",
    "\n",
    "train_dataset = SubsetWithTransform(full_dataset, train_idx, rgb_train_transform, vessel_train_transform)\n",
    "val_dataset = SubsetWithTransform(full_dataset, val_idx, rgb_val_transform, vessel_val_transform)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32 \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
    "print(f\"Number of classes: {len(set(full_dataset.labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70325f1",
   "metadata": {
    "id": "e70325f1"
   },
   "outputs": [],
   "source": [
    "# 3.5. Check Class Distribution and Add SMOTE Integration with Local Models\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "\n",
    "# Check class distribution\n",
    "class_counts = Counter(full_dataset.labels)\n",
    "print(\"Original class distribution:\")\n",
    "for class_id, count in sorted(class_counts.items()):\n",
    "    print(f\"Class {class_id}: {count} samples\")\n",
    "\n",
    "# Modified SMOTE processor with local model loading\n",
    "class SMOTEImageDatasetLocal:\n",
    "    def __init__(self, dataset, target_size=(224, 224)):\n",
    "        self.dataset = dataset\n",
    "        self.target_size = target_size\n",
    "        \n",
    "    def apply_smote(self, random_state=42, k_neighbors=5):\n",
    "        \"\"\"Apply SMOTE with local ResNet18 loading\"\"\"\n",
    "        print(\"Extracting features for SMOTE...\")\n",
    "        \n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        # Use direct torchvision model loading (no network required)\n",
    "        print(\"Loading ResNet18 model locally...\")\n",
    "        feature_extractor = models.resnet18(pretrained=True)  # Local loading\n",
    "        feature_extractor.fc = torch.nn.Identity()\n",
    "        feature_extractor.eval()\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(self.target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(len(self.dataset)):\n",
    "                rgb, vessel, label = self.dataset[i]\n",
    "                \n",
    "                if isinstance(rgb, torch.Tensor):\n",
    "                    rgb = transforms.ToPILImage()(rgb)\n",
    "                \n",
    "                img_tensor = transform(rgb).unsqueeze(0)\n",
    "                feature = feature_extractor(img_tensor).squeeze().numpy()\n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "                \n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(f\"Processed {i + 1}/{len(self.dataset)} samples\")\n",
    "        \n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        print(f\"Original distribution: {Counter(labels)}\")\n",
    "        \n",
    "        # Apply SMOTE\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        features_balanced, labels_balanced = smote.fit_resample(features, labels)\n",
    "        \n",
    "        print(f\"Balanced distribution: {Counter(labels_balanced)}\")\n",
    "        \n",
    "        return features_balanced, labels_balanced\n",
    "    \n",
    "    def create_synthetic_images(self, features_balanced, labels_balanced):\n",
    "        \"\"\"Create synthetic images from SMOTE features\"\"\"\n",
    "        synthetic_dataset = []\n",
    "        \n",
    "        for i, (feature, label) in enumerate(zip(features_balanced, labels_balanced)):\n",
    "            if i < len(self.dataset):\n",
    "                synthetic_dataset.append(self.dataset[i])\n",
    "            else:\n",
    "                # Find closest original image\n",
    "                original_features = features_balanced[:len(self.dataset)]\n",
    "                distances = np.linalg.norm(original_features - feature, axis=1)\n",
    "                closest_idx = np.argmin(distances)\n",
    "                \n",
    "                closest_rgb, closest_vessel, _ = self.dataset[closest_idx]\n",
    "                \n",
    "                if isinstance(closest_rgb, torch.Tensor):\n",
    "                    closest_rgb = transforms.ToPILImage()(closest_rgb)\n",
    "                if isinstance(closest_vessel, torch.Tensor):\n",
    "                    closest_vessel = transforms.ToPILImage()(closest_vessel)\n",
    "                \n",
    "                # Augment for synthetic samples\n",
    "                aug_transform = transforms.Compose([\n",
    "                    transforms.RandomRotation(15),\n",
    "                    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "                ])\n",
    "                \n",
    "                vessel_aug_transform = transforms.Compose([\n",
    "                    transforms.RandomRotation(15),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "                ])\n",
    "                \n",
    "                synthetic_rgb = aug_transform(closest_rgb)\n",
    "                synthetic_vessel = vessel_aug_transform(closest_vessel)\n",
    "                synthetic_dataset.append((synthetic_rgb, synthetic_vessel, label))\n",
    "        \n",
    "        return synthetic_dataset\n",
    "\n",
    "# Apply SMOTE preprocessing with local model\n",
    "print(\"\\n=== Applying SMOTE preprocessing (Local) ===\")\n",
    "smote_processor = SMOTEImageDatasetLocal(full_dataset)  # Use local version\n",
    "features_balanced, labels_balanced = smote_processor.apply_smote()\n",
    "balanced_dataset = smote_processor.create_synthetic_images(features_balanced, labels_balanced)\n",
    "\n",
    "# Check balanced distribution\n",
    "balanced_labels = [item[2] for item in balanced_dataset]\n",
    "balanced_counts = Counter(balanced_labels)\n",
    "print(\"SMOTE-balanced class distribution:\")\n",
    "for class_id, count in sorted(balanced_counts.items()):\n",
    "    print(f\"Class {class_id}: {count} samples\")\n",
    "\n",
    "# Helper class for SMOTE dataset\n",
    "class VDMDRDatasetWrapper(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "# Create wrapped dataset\n",
    "balanced_wrapped_dataset = VDMDRDatasetWrapper(balanced_dataset)\n",
    "\n",
    "# Use SMOTE-balanced data for train/val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(balanced_dataset)), \n",
    "    test_size=0.2, \n",
    "    stratify=balanced_labels, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nSMOTE dataset size: {len(balanced_dataset)}\")\n",
    "print(f\"Train: {len(train_indices)}, Val: {len(val_indices)}\")\n",
    "\n",
    "# Update datasets to use SMOTE-balanced data\n",
    "train_dataset = SubsetWithTransform(balanced_wrapped_dataset, train_indices, rgb_train_transform, vessel_train_transform)\n",
    "val_dataset = SubsetWithTransform(balanced_wrapped_dataset, val_indices, rgb_val_transform, vessel_val_transform)\n",
    "\n",
    "# Update data loaders\n",
    "BATCH_SIZE = 20  # Reduced for 384x384 training\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "print(f\"Updated Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
    "print(\"✅ SMOTE integration completed with local model loading!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fedb484",
   "metadata": {
    "id": "4fedb484"
   },
   "outputs": [],
   "source": [
    "# 4. Model Components\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Cross Attention Block\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim ** -0.5\n",
    "        \n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x, context):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        \n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.view(b, n, h, -1).transpose(1, 2), qkv)\n",
    "        \n",
    "        dots = (q @ k.transpose(-1, -2)) * self.scale\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        \n",
    "        out = (attn @ v).transpose(1, 2).reshape(b, n, -1)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# Graph Attention Pooling\n",
    "class GraphAttentionPooling(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, heads=4):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(input_dim, heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        attended, _ = self.attention(x, x, x)\n",
    "        attended = self.norm(attended + x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        pooled = attended.mean(dim=1)\n",
    "        \n",
    "        return self.fc(pooled)\n",
    "\n",
    "# Contrastive Head - FIXED VERSION\n",
    "class ContrastiveHead(nn.Module):\n",
    "    def __init__(self, input_dim, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim, proj_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        # Apply L2 normalization using F.normalize\n",
    "        return F.normalize(projected, p=2, dim=1)\n",
    "\n",
    "# Focal Loss Implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Improved Contrastive Loss\n",
    "class ImprovedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.1, margin=0.5):\n",
    "        super(ImprovedContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, features, labels):\n",
    "        # Normalize features\n",
    "        features = F.normalize(features, dim=1)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "        \n",
    "        # Create positive and negative masks\n",
    "        batch_size = features.shape[0]\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(features.device)\n",
    "        \n",
    "        # Remove diagonal (self-similarity)\n",
    "        mask = mask - torch.eye(batch_size).to(features.device)\n",
    "        \n",
    "        # Compute InfoNCE loss\n",
    "        exp_sim = torch.exp(similarity_matrix)\n",
    "        exp_sim = exp_sim * (1 - torch.eye(batch_size).to(features.device))\n",
    "        \n",
    "        pos_sim = exp_sim * mask\n",
    "        neg_sim = exp_sim * (1 - mask)\n",
    "        \n",
    "        pos_sum = pos_sim.sum(dim=1, keepdim=True)\n",
    "        neg_sum = neg_sim.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        loss = -torch.log(pos_sum / (pos_sum + neg_sum + 1e-8))\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "print(\"✅ Model components and loss functions defined successfully (L2Norm fixed)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78578e",
   "metadata": {
    "id": "dc78578e"
   },
   "outputs": [],
   "source": [
    "# 5. Fixed 384x384 VDMDR Model with Swin Transformer\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VDMDRModelEnhanced(nn.Module):\n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use Swin-Base with window12_384 for native 384x384 support\n",
    "        self.rgb_backbone = timm.create_model('swin_base_patch4_window12_384', pretrained=pretrained, num_classes=0)\n",
    "        self.vessel_backbone = timm.create_model('swin_base_patch4_window12_384', pretrained=pretrained, num_classes=0, in_chans=1)\n",
    "        \n",
    "        embed_dim = self.rgb_backbone.num_features\n",
    "        \n",
    "        # Reduced projections for memory efficiency\n",
    "        self.rgb_proj = nn.Linear(embed_dim, embed_dim // 2)\n",
    "        self.vessel_proj = nn.Linear(embed_dim, embed_dim // 2)\n",
    "        \n",
    "        # Reduced attention heads for memory efficiency\n",
    "        self.cross_attn_rv = CrossAttentionBlock(embed_dim // 2, heads=8)\n",
    "        self.cross_attn_vr = CrossAttentionBlock(embed_dim // 2, heads=8)\n",
    "        \n",
    "        # Fusion network\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.BatchNorm1d(embed_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.graph_pool = GraphAttentionPooling(embed_dim // 2, embed_dim // 2, heads=4)\n",
    "        self.contrastive_head = ContrastiveHead(embed_dim // 2, proj_dim=128)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim // 2, embed_dim // 4),\n",
    "            nn.BatchNorm1d(embed_dim // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(embed_dim // 4, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, rgb, vessel):\n",
    "        # Force resize all inputs to 384x384 to match model expectations\n",
    "        rgb = F.interpolate(rgb, size=(384, 384), mode='bilinear', align_corners=False)\n",
    "        vessel = F.interpolate(vessel, size=(384, 384), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Extract features - now guaranteed to be 384x384\n",
    "        rgb_feat = self.rgb_backbone(rgb)\n",
    "        vessel_feat = self.vessel_backbone(vessel)\n",
    "\n",
    "        # Project features (reduced dimension)\n",
    "        rgb_feat = self.rgb_proj(rgb_feat)\n",
    "        vessel_feat = self.vessel_proj(vessel_feat)\n",
    "\n",
    "        # Cross-attention (both directions)\n",
    "        rgb_seq = rgb_feat.unsqueeze(1)\n",
    "        vessel_seq = vessel_feat.unsqueeze(1)\n",
    "\n",
    "        rgb_attended = self.cross_attn_rv(rgb_seq, vessel_seq).squeeze(1)\n",
    "        vessel_attended = self.cross_attn_vr(vessel_seq, rgb_seq).squeeze(1)\n",
    "\n",
    "        # Fusion\n",
    "        fused = torch.cat([rgb_attended, vessel_attended], dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "\n",
    "        # Graph pooling\n",
    "        pooled = self.graph_pool(fused.unsqueeze(1))\n",
    "\n",
    "        # Outputs\n",
    "        contrastive_vec = self.contrastive_head(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "\n",
    "        return logits, contrastive_vec\n",
    "\n",
    "print(\"✅ Fixed 384x384 Swin-based VDMDRModelEnhanced class defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888ce9a4",
   "metadata": {
    "id": "888ce9a4"
   },
   "outputs": [],
   "source": [
    "# 6. Model Configuration and Initialization for 384x384\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Model initialization with Enhanced Model\n",
    "num_classes = len(set(full_dataset.labels))\n",
    "model = VDMDRModelEnhanced(num_classes=num_classes).to(device)\n",
    "print(f\"Enhanced model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Optimizer with adjusted learning rate for higher resolution\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=1e-6,  # Slightly lower LR for 384x384 stability\n",
    "    weight_decay=0.01, \n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Scheduler with longer cycle for more epochs\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=45, eta_min=1e-7)\n",
    "\n",
    "# Loss functions\n",
    "focal_loss = FocalLoss(gamma=2.0)\n",
    "contrastive_loss = ImprovedContrastiveLoss()\n",
    "\n",
    "print(\"✅ Enhanced model and optimizers initialized for 384x384 training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8bb2e",
   "metadata": {
    "id": "83f8bb2e"
   },
   "outputs": [],
   "source": [
    "# 6.2. MixUp Augmentation Utility\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def mixup_data(x1, x2, y, alpha=0.4):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x1.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x1.device)\n",
    "    mixed_x1 = lam * x1 + (1 - lam) * x1[index, :]\n",
    "    mixed_x2 = lam * x2 + (1 - lam) * x2[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x1, mixed_x2, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89950133",
   "metadata": {
    "id": "89950133"
   },
   "outputs": [],
   "source": [
    "# 6.3. Label Smoothing Loss\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = self.log_softmax(x)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logprobs)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * logprobs, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5bb2cd",
   "metadata": {
    "id": "0f5bb2cd"
   },
   "outputs": [],
   "source": [
    "# 6.5. Learning Rate Warmup and Advanced Scheduling\n",
    "class WarmupScheduler:\n",
    "    def __init__(self, optimizer, warmup_steps, base_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.base_lr = base_lr\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        if self.current_step <= self.warmup_steps:\n",
    "            lr = self.base_lr * (self.current_step / self.warmup_steps)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        return self.current_step <= self.warmup_steps\n",
    "\n",
    "# Initialize warmup\n",
    "warmup_scheduler = WarmupScheduler(optimizer, warmup_steps=len(train_loader) * 5, base_lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab8499-1ef7-43b1-a6ee-992920eb7158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progressive Resizing Training Strategy for RTX 5090\n",
    "class ProgressiveResizeTrainer:\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "        # Memory-optimized progressive sizes for RTX 5090\n",
    "        self.resize_schedule = [\n",
    "            {'size': 224, 'epochs': 15, 'batch_size': 32},   # Reduced from 64\n",
    "            {'size': 288, 'epochs': 20, 'batch_size': 24},   # Reduced from 48\n",
    "            {'size': 384, 'epochs': 15, 'batch_size': 16},   # Reduced from 32\n",
    "        ]\n",
    "    \n",
    "    def update_transforms(self, size):\n",
    "        \"\"\"Update dataset transforms for new image size\"\"\"\n",
    "        rgb_train_transform = transforms.Compose([\n",
    "            transforms.Resize((size, size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.6),\n",
    "            transforms.RandomVerticalFlip(p=0.3),\n",
    "            transforms.RandomRotation(45),\n",
    "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "            transforms.RandomErasing(p=0.3, scale=(0.02, 0.15))\n",
    "        ])\n",
    "        \n",
    "        rgb_val_transform = transforms.Compose([\n",
    "            transforms.Resize((size, size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "        ])\n",
    "        \n",
    "        vessel_train_transform = transforms.Compose([\n",
    "            transforms.Resize((size, size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.6),\n",
    "            transforms.RandomVerticalFlip(p=0.3),\n",
    "            transforms.RandomRotation(45),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "            transforms.RandomErasing(p=0.3, scale=(0.02, 0.15))\n",
    "        ])\n",
    "        \n",
    "        vessel_val_transform = transforms.Compose([\n",
    "            transforms.Resize((size, size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "        \n",
    "        return rgb_train_transform, rgb_val_transform, vessel_train_transform, vessel_val_transform\n",
    "    \n",
    "    def create_data_loaders(self, train_dataset, val_dataset, config):\n",
    "        \"\"\"Create data loaders with updated batch size and transforms\"\"\"\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=config['batch_size'], \n",
    "            shuffle=True, \n",
    "            num_workers=8,  # More workers for RTX 5090 system\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=config['batch_size'], \n",
    "            shuffle=False, \n",
    "            num_workers=8,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        return train_loader, val_loader\n",
    "\n",
    "print(\"Progressive Resizing trainer ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf546d9-39ef-4b1e-803b-3e47572f5b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CutMix Implementation for Medical Imaging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CutMixAugmentation:\n",
    "    def __init__(self, alpha=1.0, prob=0.5):\n",
    "        self.alpha = alpha\n",
    "        self.prob = prob\n",
    "    \n",
    "    def cutmix_data(self, rgb, vessel, target, alpha=1.0):\n",
    "        \"\"\"Apply CutMix to both RGB and vessel images\"\"\"\n",
    "        if alpha > 0:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "        \n",
    "        batch_size = rgb.size(0)\n",
    "        index = torch.randperm(batch_size).to(rgb.device)\n",
    "        \n",
    "        target_a = target\n",
    "        target_b = target[index]\n",
    "        \n",
    "        # Generate bounding box\n",
    "        W, H = rgb.size(2), rgb.size(3)\n",
    "        cut_rat = np.sqrt(1. - lam)\n",
    "        cut_w = int(W * cut_rat)\n",
    "        cut_h = int(H * cut_rat)\n",
    "        \n",
    "        # Uniform sampling for center point\n",
    "        cx = np.random.randint(W)\n",
    "        cy = np.random.randint(H)\n",
    "        \n",
    "        # Bounding box coordinates\n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "        \n",
    "        # Apply CutMix to both RGB and vessel images\n",
    "        rgb[:, :, bbx1:bbx2, bby1:bby2] = rgb[index, :, bbx1:bbx2, bby1:bby2]\n",
    "        vessel[:, :, bbx1:bbx2, bby1:bby2] = vessel[index, :, bbx1:bbx2, bby1:bby2]\n",
    "        \n",
    "        # Adjust lambda to match pixel ratio\n",
    "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "        \n",
    "        return rgb, vessel, target_a, target_b, lam\n",
    "\n",
    "def cutmix_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"CutMix loss calculation\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "print(\"CutMix augmentation ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad07bb-7ccc-4f0b-b95b-b0608b5b16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.8. Memory Management for RTX 5090\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(f\"GPU Memory cleared. Available: {torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated():.2f} bytes\")\n",
    "\n",
    "def get_gpu_memory_info():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3    # GB\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB, Total: {total:.2f}GB\")\n",
    "\n",
    "# Clear memory before starting\n",
    "clear_gpu_memory()\n",
    "get_gpu_memory_info()\n",
    "\n",
    "# Update CUDA memory settings\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512,expandable_segments:True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca02833",
   "metadata": {
    "id": "fca02833"
   },
   "outputs": [],
   "source": [
    "# 7. Training Loop: Mixed Precision, Gradient Accumulation\n",
    "\n",
    "# 7. Training Loop: Mixed Precision, Gradient Accumulation, MixUp\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, scaler, epoch, use_mixup=True):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "\n",
    "    for batch_idx, (rgb, vessel, labels) in enumerate(pbar):\n",
    "        rgb, vessel, labels = rgb.to(device), vessel.to(device), labels.to(device)\n",
    "\n",
    "        # MixUp augmentation\n",
    "        if use_mixup:\n",
    "            rgb, vessel, y_a, y_b, lam = mixup_data(rgb, vessel, labels)\n",
    "        else:\n",
    "            y_a, y_b, lam = labels, labels, 1.0\n",
    "\n",
    "        with autocast():\n",
    "            logits, contrastive_vec = model(rgb, vessel)\n",
    "            # Use only classification loss for MixUp\n",
    "            loss = mixup_criterion(nn.CrossEntropyLoss(), logits, y_a, y_b, lam)\n",
    "            # Optionally add contrastive loss if you want\n",
    "            # loss += 0.1 * ImprovedContrastiveLoss()(contrastive_vec, y_a)\n",
    "            loss = loss / ACCUMULATION_STEPS\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * ACCUMULATION_STEPS\n",
    "        _, predicted = logits.max(1)\n",
    "        total += labels.size(0)\n",
    "        # For accuracy, use y_a (original labels)\n",
    "        correct += predicted.eq(y_a).sum().item()\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{total_loss/(batch_idx+1):.4f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "\n",
    "    scheduler.step()\n",
    "    return total_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rgb, vessel, labels in tqdm(val_loader, desc='Validation'):\n",
    "            rgb, vessel, labels = rgb.to(device), vessel.to(device), labels.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                logits, contrastive_vec = model(rgb, vessel)\n",
    "                loss = combined_loss(logits, contrastive_vec, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(val_loader), 100. * correct / total, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d61b77",
   "metadata": {
    "id": "64d61b77"
   },
   "outputs": [],
   "source": [
    "def train_stable_384():\n",
    "    \"\"\"Ultra-stable 384x384 training with all safeguards\"\"\"\n",
    "    \n",
    "    def stable_loss_fn(logits, contrastive_vec, labels):\n",
    "        # Clamp logits for stability\n",
    "        logits = torch.clamp(logits, min=-10, max=10)\n",
    "        \n",
    "        # Simple cross-entropy (most stable)\n",
    "        ce_loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        # Check for NaN/inf\n",
    "        if not torch.isfinite(ce_loss):\n",
    "            print(\"⚠️ Loss instability detected, using dummy loss\")\n",
    "            return torch.tensor(1.0, requires_grad=True, device=logits.device)\n",
    "        \n",
    "        return ce_loss\n",
    "    \n",
    "    # Ultra-conservative learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-6, weight_decay=0.01)  # Even more conservative\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(45):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (rgb, vessel, labels) in enumerate(train_loader_384):\n",
    "            rgb, vessel, labels = rgb.to(device), vessel.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass (no mixed precision)\n",
    "            logits, contrastive_vec = model(rgb, vessel)\n",
    "            loss = stable_loss_fn(logits, contrastive_vec, labels)\n",
    "            \n",
    "            # Check for instability\n",
    "            if not torch.isfinite(loss):\n",
    "                print(f\"⚠️ Skipping batch {batch_idx} due to unstable loss\")\n",
    "                continue\n",
    "            \n",
    "            # Backward pass with gradient clipping\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # Very conservative\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_acc = 100. * train_correct / train_total if train_total > 0 else 0\n",
    "        print(f'Epoch {epoch+1}: Train Acc: {train_acc:.2f}%')\n",
    "        \n",
    "        if train_acc > best_acc:\n",
    "            best_acc = train_acc\n",
    "    \n",
    "    return best_acc\n",
    "\n",
    "# Run stable training\n",
    "final_accuracy = train_stable_384()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ff0a8-f924-4266-a995-9a04170c3474",
   "metadata": {
    "id": "ac7ff0a8-f924-4266-a995-9a04170c3474"
   },
   "outputs": [],
   "source": [
    "# Reload the best model weights (before ensemble training)\n",
    "model = VDMDRModel(num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load('best_vdmdr_model.pth', map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b02a6",
   "metadata": {
    "id": "e10b02a6"
   },
   "outputs": [],
   "source": [
    "# 8.5. Model Ensemble for Higher Accuracy (memory-safe, reduced batch size)\n",
    "\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "\n",
    "# Reduce batch size for ensemble training to save memory\n",
    "ENSEMBLE_BATCH_SIZE = 8\n",
    "ensemble_train_loader = DataLoader(train_dataset, batch_size=ENSEMBLE_BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "ensemble_val_loader = DataLoader(val_dataset, batch_size=ENSEMBLE_BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super().__init__()\n",
    "        self.models = models\n",
    "\n",
    "    def forward(self, rgb, vessel):\n",
    "        logits_list = []\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            logits, _ = model(rgb, vessel)\n",
    "            logits_list.append(logits)\n",
    "        avg_logits = torch.stack(logits_list).mean(0)\n",
    "        return avg_logits\n",
    "\n",
    "ensemble_models = [model.to('cpu')]  # Move best model to CPU to save GPU memory\n",
    "\n",
    "for seed in [123, 456, 789]:\n",
    "    print(f\"Training ensemble model with seed {seed}...\")\n",
    "    torch.manual_seed(seed)\n",
    "    ensemble_model = VDMDRModel(num_classes=num_classes).to(device)\n",
    "    ensemble_optimizer = optim.AdamW(ensemble_model.parameters(), lr=2e-4, weight_decay=0.01)\n",
    "    ensemble_scheduler = lr_scheduler.CosineAnnealingLR(ensemble_optimizer, T_max=30, eta_min=1e-6)\n",
    "    for epoch in range(20):\n",
    "        train_loss, train_acc = train_epoch(ensemble_model, ensemble_train_loader, ensemble_optimizer, ensemble_scheduler, scaler, epoch)\n",
    "        val_loss, val_acc, _, _ = validate(ensemble_model, ensemble_val_loader)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Seed {seed} - Epoch {epoch}: Val Acc: {val_acc:.2f}%')\n",
    "    # Move model to CPU and force memory release\n",
    "    ensemble_model_cpu = ensemble_model.to('cpu')\n",
    "    del ensemble_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    ensemble_models.append(ensemble_model_cpu)\n",
    "\n",
    "ensemble = EnsembleModel(ensemble_models)\n",
    "print(f\"Created ensemble with {len(ensemble_models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4554f1-d093-4e96-be14-d49a776ea40e",
   "metadata": {
    "id": "bd4554f1-d093-4e96-be14-d49a776ea40e"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fefc072",
   "metadata": {
    "id": "2fefc072"
   },
   "outputs": [],
   "source": [
    "# 9. Feature Extraction & XGBoost Ensemble\n",
    "\n",
    "def extract_features(model, dataloader):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for rgb, vessel, batch_labels in tqdm(dataloader, desc='Extracting features'):\n",
    "            rgb, vessel = rgb.to(device), vessel.to(device)\n",
    "            # No autocast for feature extraction\n",
    "            logits, contrastive_vec = model(rgb, vessel)\n",
    "            combined_features = torch.cat([logits, contrastive_vec], dim=1)\n",
    "            features.append(combined_features.cpu().numpy())\n",
    "            labels.extend(batch_labels.numpy())\n",
    "    return np.vstack(features), np.array(labels)\n",
    "\n",
    "# Extract features\n",
    "print(\"Extracting training features...\")\n",
    "train_features, train_labels = extract_features(model, train_loader)\n",
    "print(\"Extracting validation features...\")\n",
    "val_features, val_labels = extract_features(model, val_loader)\n",
    "\n",
    "# XGBoost ensemble\n",
    "print(\"Training XGBoost ensemble...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(train_features, train_labels)\n",
    "xgb_preds = xgb_model.predict(val_features)\n",
    "xgb_acc = 100. * (xgb_preds == val_labels).sum() / len(val_labels)\n",
    "print(f\"XGBoost ensemble accuracy: {xgb_acc:.2f}%\")\n",
    "\n",
    "# Save XGBoost model\n",
    "import pickle\n",
    "with open('xgb_ensemble.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f01b6cb",
   "metadata": {
    "id": "0f01b6cb"
   },
   "outputs": [],
   "source": [
    "# 9.5. Optuna Hyperparameter Tuning for XGBoost Ensemble\n",
    "\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.3),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'mlogloss'\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(train_features, train_labels)\n",
    "    preds = model.predict(val_features)\n",
    "    acc = accuracy_score(val_labels, preds)\n",
    "    return acc\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"Best XGBoost params:\", study.best_params)\n",
    "print(\"Best XGBoost validation accuracy: {:.2f}%\".format(100 * study.best_value))\n",
    "\n",
    "# Train and save the best XGBoost model\n",
    "best_xgb = xgb.XGBClassifier(**study.best_params)\n",
    "best_xgb.fit(train_features, train_labels)\n",
    "optuna_preds = best_xgb.predict(val_features)\n",
    "optuna_acc = accuracy_score(val_labels, optuna_preds)\n",
    "print(f\"Optuna-tuned XGBoost accuracy: {optuna_acc*100:.2f}%\")\n",
    "\n",
    "import pickle\n",
    "with open('xgb_ensemble_optuna.pkl', 'wb') as f:\n",
    "    pickle.dump(best_xgb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f1fbc",
   "metadata": {
    "id": "db6f1fbc"
   },
   "outputs": [],
   "source": [
    "def tta_predict(model, rgb, vessel, n_augments=5):\n",
    "    \"\"\"Test Time Augmentation - applies augmentations during inference\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    # Original prediction\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(rgb, vessel)\n",
    "        predictions.append(torch.softmax(logits, dim=1))\n",
    "\n",
    "    # Augmented predictions\n",
    "    tta_transforms = [\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        transforms.RandomRotation(10),\n",
    "        lambda x: transforms.functional.rotate(x, -10),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    ]\n",
    "\n",
    "    for transform in tta_transforms:\n",
    "        if callable(transform):\n",
    "            rgb_aug = transform(rgb)\n",
    "            vessel_aug = transform(vessel)\n",
    "        else:\n",
    "            rgb_aug = transform(rgb)\n",
    "            vessel_aug = transform(vessel)\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(rgb_aug, vessel_aug)\n",
    "            predictions.append(torch.softmax(logits, dim=1))\n",
    "\n",
    "    # Average predictions\n",
    "    avg_pred = torch.stack(predictions).mean(0)\n",
    "    return avg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e28833-03ca-4ec3-b5bd-57dfc55d69bf",
   "metadata": {
    "id": "a4e28833-03ca-4ec3-b5bd-57dfc55d69bf"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f45700",
   "metadata": {
    "id": "77f45700"
   },
   "outputs": [],
   "source": [
    "# 10. Evaluation & Reporting\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "final_preds = []\n",
    "final_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for rgb, vessel, labels in tqdm(val_loader, desc='Final evaluation'):\n",
    "        rgb, vessel, labels = rgb.to(device), vessel.to(device), labels.to(device)\n",
    "\n",
    "        with autocast():\n",
    "            logits, _ = model(rgb, vessel)\n",
    "\n",
    "        _, predicted = logits.max(1)\n",
    "        final_preds.extend(predicted.cpu().numpy())\n",
    "        final_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(final_labels, final_preds))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(final_labels, final_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train Acc')\n",
    "plt.plot(val_accs, label='Val Acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Results:\")\n",
    "print(f\"Best Validation Accuracy: {best_acc:.2f}%\")\n",
    "print(f\"XGBoost Ensemble Accuracy: {xgb_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
